2025-07-23 13:31:25.657513: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-07-23 13:31:25.671823: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1753277485.689102   35547 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1753277485.694730   35547 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1753277485.709484   35547 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1753277485.709587   35547 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1753277485.709654   35547 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1753277485.709711   35547 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-07-23 13:31:25.713573: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-07-23 13:31:29,098 - __main__ - INFO - Stage Progression Validator initialized
2025-07-23 13:31:29,098 - __main__ - INFO - Validating bluebook model for Stage 0 progression
2025-07-23 13:31:29,461 - train_grpo - INFO - GRPO Legal Trainer initialized
2025-07-23 13:31:29,462 - train_grpo - INFO - Model path: Qwen/Qwen3-14B (14B)
2025-07-23 13:31:29,462 - train_grpo - INFO - Task: bluebook
2025-07-23 13:31:29,462 - train_grpo - INFO - Device: cuda
2025-07-23 13:31:29,462 - train_grpo - INFO - Output directory: models/grpo/qwen3-14b/grpo/bluebook
2025-07-23 13:31:29,462 - train_grpo - INFO - Starting GRPO training...
2025-07-23 13:31:29,464 - train_grpo - INFO - Loading model and tokenizer...
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|â–ˆâ–Ž        | 1/8 [00:00<00:05,  1.27it/s]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:01<00:04,  1.23it/s]Loading checkpoint shards:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:02<00:04,  1.21it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:03<00:03,  1.20it/s]Loading checkpoint shards:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:04<00:02,  1.20it/s]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [00:04<00:01,  1.20it/s]Loading checkpoint shards:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:05<00:00,  1.20it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:06<00:00,  1.46it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:06<00:00,  1.29it/s]
2025-07-23 13:31:36,861 - train_grpo - INFO - Model and tokenizer loaded successfully
2025-07-23 13:31:36,861 - train_grpo - INFO - Loading training dataset from: kylebrussell/cap-rlvr-bluebook
Repo card metadata block was not found. Setting CardData to empty.
2025-07-23 13:31:37,082 - huggingface_hub.repocard - WARNING - Repo card metadata block was not found. Setting CardData to empty.
2025-07-23 13:31:46,953 - train_grpo - INFO - Preparing dataset for GRPO training...
2025-07-23 13:31:47,496 - train_grpo - INFO - Prepared 252670 samples for training
2025-07-23 13:31:48,265 - train_grpo - INFO - Using training dataset as evaluation dataset for eval-only mode
2025-07-23 13:31:48,268 - train_grpo - INFO - GRPO Config: {
  "output_dir": "models/grpo/qwen3-14b/grpo/bluebook/bluebook_grpo",
  "per_device_train_batch_size": 2,
  "per_device_eval_batch_size": 4,
  "num_train_epochs": 3,
  "learning_rate": 1e-05,
  "weight_decay": 0.01,
  "warmup_steps": 100,
  "gradient_accumulation_steps": 8,
  "dataloader_num_workers": 4,
  "remove_unused_columns": false,
  "save_strategy": "steps",
  "save_steps": 500,
  "eval_strategy": "steps",
  "eval_steps": 500,
  "logging_strategy": "steps",
  "logging_steps": 50,
  "load_best_model_at_end": true,
  "metric_for_best_model": "eval/rewards/mean",
  "greater_is_better": true,
  "seed": 42,
  "data_seed": 42,
  "bf16": true,
  "report_to": null,
  "push_to_hub": false,
  "temperature": 0.7,
  "beta": 0.1,
  "num_generations": 4,
  "max_prompt_length": 1024,
  "max_completion_length": 512
}
2025-07-23 13:31:48,297 - train_grpo - INFO - Initializing GRPO trainer...
[2025-07-23 13:31:48,691] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-07-23 13:31:49,926] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|â–ˆâ–Ž        | 1/8 [00:00<00:03,  2.33it/s]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:00<00:02,  2.50it/s]Loading checkpoint shards:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:01<00:01,  2.58it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:01<00:01,  2.60it/s]Loading checkpoint shards:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:01<00:01,  2.64it/s]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [00:02<00:00,  2.68it/s]Loading checkpoint shards:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:02<00:00,  2.76it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:02<00:00,  3.36it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:02<00:00,  2.87it/s]
2025-07-23 13:32:00,652 - train_grpo - INFO - Running evaluation only...
`generation_config` default values have been modified to match model-specific defaults: {'top_p': 0.95, 'bos_token_id': 151643}. If this is not desired, please set these values explicitly.
2025-07-23 13:32:21,683 - train_grpo - ERROR - Evaluation failed: CUDA out of memory. Tried to allocate 72.00 MiB. GPU 0 has a total capacity of 79.19 GiB of which 9.00 MiB is free. Including non-PyTorch memory, this process has 79.17 GiB memory in use. Of the allocated memory 78.14 GiB is allocated by PyTorch, and 379.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-07-23 13:32:21,684 - __main__ - ERROR - Error validating bluebook: CUDA out of memory. Tried to allocate 72.00 MiB. GPU 0 has a total capacity of 79.19 GiB of which 9.00 MiB is free. Including non-PyTorch memory, this process has 79.17 GiB memory in use. Of the allocated memory 78.14 GiB is allocated by PyTorch, and 379.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
GRPO Stage 0 Progression Validation Report
==================================================
Stage: 0 - Individual Task Mastery
Threshold: 80.0% reward

Overall Status: 0/1 models passed
Progression Ready: âŒ NO

âŒ bluebook: ERROR
   Error: CUDA out of memory. Tried to allocate 72.00 MiB. GPU 0 has a total capacity of 79.19 GiB of which 9.00 MiB is free. Including non-PyTorch memory, this process has 79.17 GiB memory in use. Of the allocated memory 78.14 GiB is allocated by PyTorch, and 379.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

ðŸ“‹ Next Steps:
   1. Address failing validations above
   2. Continue training with recommended adjustments
   3. Re-run validation when ready
